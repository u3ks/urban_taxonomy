{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b93f3ae2-1b38-4fd8-896f-305c680d00ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import numba\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from libpysal.graph import read_parquet\n",
    "from sklearn.preprocessing import PowerTransformer, RobustScaler, StandardScaler\n",
    "import momepy as mm\n",
    "\n",
    "regions_datadir = \"/data/uscuni-ulce/\"\n",
    "data_dir = \"/data/uscuni-ulce/processed_data/\"\n",
    "eubucco_files = glob.glob(regions_datadir + \"eubucco_raw/*\")\n",
    "graph_dir = data_dir + \"neigh_graphs/\"\n",
    "chars_dir = \"/data/uscuni-ulce/processed_data/chars/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd7e3178-770b-4dba-985d-414574f5dc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.cluster_validation import generate_enc_groups\n",
    "from core.utils import used_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3c030d3b-c4ec-4533-a977-ecfb4ccf632c",
   "metadata": {},
   "outputs": [],
   "source": [
    "region_hulls = gpd.read_parquet(\n",
    "        regions_datadir + \"regions/\" + \"regions_hull.parquet\"\n",
    "    ).to_crs('epsg:4326')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0610f843-0bc3-41b4-93d7-e01be302a3c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69300"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for region_id, region_hull in region_hulls.iterrows():\n",
    "    region_hull = region_hull[\"convex_hull\"]\n",
    "    if region_id == 69300: break\n",
    "region_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1d77ae90-733f-4a13-930f-fe12d9d549bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "hull_boundary = region_hull.bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd23cbc-700a-4896-bc89-6277e28f45f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9d8d20fc-058c-4514-b419-e2974f08a380",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !conda install -c conda-forge overturemaps -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "06af8c88-6343-444c-b605-4bcf366a9453",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "\n",
    "import pyarrow as pa\n",
    "import pyarrow.compute as pc\n",
    "import pyarrow.dataset as ds\n",
    "import pyarrow.fs as fs\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "from typing import Optional\n",
    "import pyarrow.parquet as pq\n",
    "import shapely.wkb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4f00362b-2a8d-4335-8e4b-0d51b7808c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_batch_reader(overture_type, bbox=None) -> Optional[pa.RecordBatchReader]:\n",
    "    \"\"\"\n",
    "    Return a pyarrow RecordBatchReader for the desired bounding box and s3 path\n",
    "    \"\"\"\n",
    "    path = _dataset_path(overture_type)\n",
    "\n",
    "    if bbox:\n",
    "        xmin, ymin, xmax, ymax = bbox\n",
    "        filter = (\n",
    "            (pc.field(\"bbox\", \"xmin\") < xmax)\n",
    "            & (pc.field(\"bbox\", \"xmax\") > xmin)\n",
    "            & (pc.field(\"bbox\", \"ymin\") < ymax)\n",
    "            & (pc.field(\"bbox\", \"ymax\") > ymin)\n",
    "        )\n",
    "    else:\n",
    "        filter = None\n",
    "\n",
    "    dataset = ds.dataset(\n",
    "        path, filesystem=fs.S3FileSystem(anonymous=True, region=\"us-west-2\")\n",
    "    )\n",
    "    batches = dataset.to_batches(filter=filter)\n",
    "\n",
    "    # to_batches() can yield many batches with no rows. I've seen\n",
    "    # this cause downstream crashes or other negative effects. For\n",
    "    # example, the ParquetWriter will emit an empty row group for\n",
    "    # each one bloating the size of a parquet file. Just omit\n",
    "    # them so the RecordBatchReader only has non-empty ones. Use\n",
    "    # the generator syntax so the batches are streamed out\n",
    "    non_empty_batches = (b for b in batches if b.num_rows > 0)\n",
    "\n",
    "    geoarrow_schema = geoarrow_schema_adapter(dataset.schema)\n",
    "    reader = pa.RecordBatchReader.from_batches(geoarrow_schema, non_empty_batches)\n",
    "    return reader\n",
    "\n",
    "\n",
    "def geoarrow_schema_adapter(schema: pa.Schema) -> pa.Schema:\n",
    "    \"\"\"\n",
    "    Convert a geoarrow-compatible schema to a proper geoarrow schema\n",
    "\n",
    "    This assumes there is a single \"geometry\" column with WKB formatting\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    schema: pa.Schema\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pa.Schema\n",
    "    A copy of the input schema with the geometry field replaced with\n",
    "    a new one with the proper geoarrow ARROW:extension metadata\n",
    "\n",
    "    \"\"\"\n",
    "    geometry_field_index = schema.get_field_index(\"geometry\")\n",
    "    geometry_field = schema.field(geometry_field_index)\n",
    "    geoarrow_geometry_field = geometry_field.with_metadata(\n",
    "        {b\"ARROW:extension:name\": b\"geoarrow.wkb\"}\n",
    "    )\n",
    "\n",
    "    geoarrow_schema = schema.set(geometry_field_index, geoarrow_geometry_field)\n",
    "\n",
    "    return geoarrow_schema\n",
    "\n",
    "\n",
    "type_theme_map = {\n",
    "    \"locality\": \"admins\",\n",
    "    \"locality_area\": \"admins\",\n",
    "    \"administrative_boundary\": \"admins\",\n",
    "    \"building\": \"buildings\",\n",
    "    \"building_part\": \"buildings\",\n",
    "    \"division\": \"divisions\",\n",
    "    \"division_area\": \"divisions\",\n",
    "    \"place\": \"places\",\n",
    "    \"segment\": \"transportation\",\n",
    "    \"connector\": \"transportation\",\n",
    "    \"infrastructure\": \"base\",\n",
    "    \"land\": \"base\",\n",
    "    \"land_cover\": \"base\",\n",
    "    \"land_use\": \"base\",\n",
    "    \"water\": \"base\",\n",
    "}\n",
    "\n",
    "\n",
    "def _dataset_path(overture_type: str) -> str:\n",
    "    \"\"\"\n",
    "    Returns the s3 path of the Overture dataset to use. This assumes overture_type has\n",
    "    been validated, e.g. by the CLI\n",
    "\n",
    "    \"\"\"\n",
    "    # Map of sub-partition \"type\" to parent partition \"theme\" for forming the\n",
    "    # complete s3 path. Could be discovered by reading from the top-level s3\n",
    "    # location but this allows to only read the files in the necessary partition.\n",
    "    theme = type_theme_map[overture_type]\n",
    "    return f\"overturemaps-us-west-2/release/2024-06-13-beta.1/theme={theme}/type={overture_type}/\"\n",
    "\n",
    "\n",
    "def get_all_overture_types() -> List[str]:\n",
    "    return list(type_theme_map.keys())\n",
    "\n",
    "def get_writer(output_format, path, schema):\n",
    "    if output_format == \"geojson\":\n",
    "        writer = GeoJSONWriter(path)\n",
    "    elif output_format == \"geojsonseq\":\n",
    "        writer = GeoJSONSeqWriter(path)\n",
    "    elif output_format == \"geoparquet\":\n",
    "        # Update the geoparquet metadata to remove the file-level bbox which\n",
    "        # will no longer apply to this file. Since we cannot write the field at\n",
    "        # the end, just remove it as it's optional. Let the per-row bounding\n",
    "        # boxes do all the work.\n",
    "        metadata = schema.metadata\n",
    "        geo = json.loads(metadata[b\"geo\"])\n",
    "        for column in geo[\"columns\"].values():\n",
    "            column.pop(\"bbox\")\n",
    "        metadata[b\"geo\"] = json.dumps(geo).encode(\"utf-8\")\n",
    "        schema = schema.with_metadata(metadata)\n",
    "\n",
    "        writer = pq.ParquetWriter(path, schema)\n",
    "    return writer\n",
    "\n",
    "def copy(reader, writer):\n",
    "    while True:\n",
    "        try:\n",
    "            batch = reader.read_next_batch()\n",
    "        except StopIteration:\n",
    "            break\n",
    "        if batch.num_rows > 0:\n",
    "            writer.write_batch(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8c203395-d9e0-44d6-94d0-4172fcc8dd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def download(bbox, output_format, output, type_):\n",
    "    if output is None:\n",
    "        output = sys.stdout\n",
    "\n",
    "    reader = record_batch_reader(type_, bbox)\n",
    "    if reader is None:\n",
    "        return\n",
    "\n",
    "    with get_writer(output_format, output, schema=reader.schema) as writer:\n",
    "        copy(reader, writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7605fba8-740f-4bf2-9af6-222504ea58d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 μs, sys: 0 ns, total: 2 μs\n",
      "Wall time: 4.29 μs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "type_ = 'segment'\n",
    "output_format = 'geoparquet'\n",
    "\n",
    "# download(hull_boundary, output_format, f'../data/prague_overture_{type_}.{output_format}', type_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2d089edb-5643-4b20-ab5c-1b57863631d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.35 s, sys: 1.25 s, total: 3.6 s\n",
      "Wall time: 2min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "batches = record_batch_reader(type_, hull_boundary).read_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d195e590-fcdf-459f-a739-abbf80cb156d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.GeoDataFrame.from_arrow(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ad6dc58d-394b-4717-b393-984f1a2b44dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gdf.iloc[gdf.sindex.query(region_hull, predicate='intersects')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "053cfa69-18cb-4a07-adc7-9f9630ef89dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## service road removed\n",
    "query = \"living_street|motorway|motorway_link|pedestrian|primary|primary_link|residential|secondary|secondary_link|tertiary|tertiary_link|trunk|trunk_link|unclassified\"\n",
    "approved_roads = query.split('|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c169b25c-4104-4ee8-9aca-a770dc3bc279",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gdf[gdf['class'].isin(approved_roads)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e813825a-d813-4f4f-a12d-526ecaed460c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gdf.sort_values('geometry').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c3b09952-0bf5-477e-b8a2-65573df671f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.to_parquet(data_dir + f\"streets/streets_{region_id}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c913ad4-1ea2-4c76-8e5a-e35e427f0208",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39d45ae-b419-4b02-9949-b2d77b3baa70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd62c65-9720-4db1-88a3-f91d89149792",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c2bee589-2b3c-4a9a-84da-047466ea2bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "streets = gpd.read_parquet(data_dir + f\"/streets/streets_{region_id}.parquet\")\n",
    "\n",
    "graph = mm.gdf_to_nx(streets)\n",
    "graph = mm.node_degree(graph)\n",
    "graph = mm.subgraph(\n",
    "    graph,\n",
    "    radius=5,\n",
    "    meshedness=True,\n",
    "    cds_length=False,\n",
    "    mode=\"sum\",\n",
    "    degree=\"degree\",\n",
    "    length=\"mm_len\",\n",
    "    mean_node_degree=False,\n",
    "    proportion={0: True, 3: True, 4: True},\n",
    "    cyclomatic=False,\n",
    "    edge_node_ratio=False,\n",
    "    gamma=False,\n",
    "    local_closeness=True,\n",
    "    closeness_weight=\"mm_len\",\n",
    "    node_density=True,\n",
    "    verbose=False,\n",
    ")\n",
    "graph = mm.cds_length(graph, radius=3, name=\"ldsCDL\", verbose=False)\n",
    "graph = mm.clustering(graph, name=\"xcnSCl\")\n",
    "graph = mm.mean_node_dist(graph, name=\"mtdMDi\", verbose=False)\n",
    "\n",
    "nodes, edges = mm.nx_to_gdf(graph, spatial_weights=False)\n",
    "edges = edges.sort_values('geometry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27668a6c-12d6-4612-bc40-81b1b908a9a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9d57cced-1daa-439e-86c7-26a0c084b205",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        0.039749\n",
       "3        0.016040\n",
       "6        0.004324\n",
       "10       0.006531\n",
       "8        0.008161\n",
       "           ...   \n",
       "32374    0.000259\n",
       "32373    0.000317\n",
       "59858    0.000032\n",
       "32371    0.002135\n",
       "59857    0.001264\n",
       "Length: 59859, dtype: float64"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edges.geometry.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "eef0fd75-3fa6-40d9-9c03-0e28a341bf48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        0.039749\n",
       "1        0.016040\n",
       "2        0.004324\n",
       "3        0.006531\n",
       "4        0.008161\n",
       "           ...   \n",
       "59854    0.000259\n",
       "59855    0.000317\n",
       "59856    0.000032\n",
       "59857    0.002135\n",
       "59858    0.001264\n",
       "Length: 59859, dtype: float64"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "streets.geometry.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d136c4b-a5bd-494f-93fb-bf82ec7d30d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
